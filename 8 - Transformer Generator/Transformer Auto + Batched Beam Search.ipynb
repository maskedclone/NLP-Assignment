{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer and Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from queue import PriorityQueue\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Load data - Wiki Text\n",
    "\n",
    "We will be using wikitext which contains a large corpus of text, perfect for language modeling task.  This time, we will use the `datasets` library from HuggingFace to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ptb_text_only (/home/st122934/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65a1a294e714e3ca6d30f663999ff45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence'],\n",
      "        num_rows: 42068\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence'],\n",
      "        num_rows: 3761\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence'],\n",
      "        num_rows: 3370\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "#there are raw and preprocessed version; we used the raw one and preprocessed ourselves for fun\n",
    "dataset = datasets.load_dataset('ptb_text_only')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behind all the <unk> is some <unk> competition\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIf you try to change the index you might notice that sometimes there is no paragraph \\nand rather an empty string so we will have to care of that later.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['train'][333]['sentence'])\n",
    "\n",
    "'''\n",
    "If you try to change the index you might notice that sometimes there is no paragraph \n",
    "and rather an empty string so we will have to care of that later.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Simply tokenize the given text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/st122934/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-6d1e5992cb800354.arrow\n",
      "Loading cached processed dataset at /home/st122934/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-6fd08b61fe272feb.arrow\n",
      "Loading cached processed dataset at /home/st122934/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-b98755e1d2264d31.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['behind', 'all', 'the', '<unk>', 'is', 'some', '<unk>', 'competition']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "#function to tokenize\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['sentence'])}  \n",
    "\n",
    "#map the function to each example\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['sentence'], fn_kwargs={'tokenizer': tokenizer})\n",
    "print(tokenized_dataset['train'][333]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9881\n",
      "['<unk>', '<pad>', '<sos>', '<eos>', 'the', 'n', 'of', 'to', 'a', 'in']\n"
     ]
    }
   ],
   "source": [
    "## numericalizing\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=3, specials=special_symbols)   \n",
    "\n",
    "vocab.set_default_index(vocab['<unk>'])   \n",
    "print(len(vocab))                         \n",
    "print(vocab.get_itos()[:10])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Prepare the batch loader\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data \"Chaky loves eating at\", \"AIT `<eos>` I really\", \"love deep learning `<eos>`\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['tokens']:         \n",
    "            #appends eos so we know it ends....so model learn how to end...                             \n",
    "            tokens = example['tokens'].append('<eos>')   \n",
    "            #numericalize          \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size #get the int number of batches...\n",
    "    data = data[:num_batches * batch_size] #make the batch evenly, and cut out any remaining                      \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data #[batch size, bunch of tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am using Batched Beam Search, where instead of feeding each hypothesis one by one, which takes a lot of time;  I simply concat everything into one list and feed them all at once, which is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device, pad_idx, max_length = 100):\n",
    "                \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def make_mask(self, x):\n",
    "        \n",
    "        #x = [batch size, len]\n",
    "        \n",
    "        pad_mask = (x != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #pad_mask = [batch size, 1, 1, len]\n",
    "        \n",
    "        x_len = x.shape[1]\n",
    "        \n",
    "        sub_mask = torch.tril(torch.ones((x_len, x_len), device = self.device)).bool()\n",
    "        #sub_mask = [len, len]\n",
    "            \n",
    "        mask = pad_mask & sub_mask\n",
    "        #mask = [batch size, 1, len, len]\n",
    "        \n",
    "        return mask \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, len]\n",
    "                \n",
    "        batch_size = x.shape[0]\n",
    "        x_len      = x.shape[1]\n",
    "        \n",
    "        #get mask here since we remove seq2seq class\n",
    "        mask   = self.make_mask(x)\n",
    "        #mask = [batch size, 1, len, len]\n",
    "\n",
    "        pos = torch.arange(0, x_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)          \n",
    "            \n",
    "        x = self.dropout((self.tok_embedding(x) * self.scale) + self.pos_embedding(pos))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, attention = layer(x, mask)\n",
    "        \n",
    "        #x = [batch size, len, hid dim]\n",
    "        #attention = [batch size, n heads, len, len]\n",
    "        \n",
    "        output = self.fc_out(x)\n",
    "        #output = [batch size, len, output dim]\n",
    "            \n",
    "        return output, attention\n",
    "\n",
    "    def beam_decode(self, penalty_alpha = 0.9, max_length = 5, beam_size = 5):\n",
    "        \n",
    "        # Start with SOS Harry Potter is\n",
    "        prompt = 'Harry Potter is '\n",
    "        \n",
    "        tokens = tokenizer(prompt)\n",
    "        indices = [SOS_IDX] + [vocab[t] for t in tokens]\n",
    "\n",
    "        decoder_input = torch.Tensor([indices]).long().to(device)\n",
    "        #decoder_input: [batch size, len] = [1, 1]\n",
    "        scores = torch.Tensor([0.]).to(device)\n",
    "        #scores: [1]\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            \n",
    "            # print(f\"========Length: {i}\")\n",
    "            \n",
    "            # Decoder prediction\n",
    "            logits, _ = self.forward(decoder_input)\n",
    "            #[beam_size, current dec len=i, vocab_size]\n",
    "                        \n",
    "            logits = logits[:, -1] \n",
    "            # Last sequence step: [beam_size, current dec len=i, vocab_size] => [beam_size, vocab_size]\n",
    "            \n",
    "            # print(f\"{logits.shape=}\")\n",
    "\n",
    "            # Softmax\n",
    "            # Log softmax is better, since beam search accumulates probability\n",
    "            # if simply softmax, the probability can get too small and then become unstable\n",
    "            log_probs = torch.log_softmax(logits, dim=1)\n",
    "    \n",
    "            # Add length penalty, otherwise, always very short sentence will win...\n",
    "            penalty   = ((5 + (i+1)) / (5 + 1)) ** penalty_alpha #see https://arxiv.org/abs/1609.08144\n",
    "            log_probs = log_probs / penalty\n",
    "            \n",
    "            # print(f\"{decoder_input[:, -1]=}\")\n",
    "            \n",
    "            # Update score where EOS has not been reached\n",
    "            log_probs[decoder_input[:, -1]==EOS_IDX, :] = -2 #discouraged it to end\n",
    "            log_probs[decoder_input[:, -1]==UNK_IDX, :] = -10 #very discouraged to spit out unk\n",
    "            scores = scores.unsqueeze(1) + log_probs \n",
    "            # scores: [beam_size, vocab_size]\n",
    "            # log_probs: [beam_size, vocab_size]\n",
    "\n",
    "            # print(f\"{log_probs.shape=}\")\n",
    "            # print(f\"{scores.shape=}\")\n",
    "            #log_probs: torch.Size([1, 29475])\n",
    "            #scores.shape=torch.Size([1, 29475])\n",
    "            \n",
    "            # Flatten scores from [beams, vocab_size] to [beams * vocab_size] to get top k, and reconstruct beam indices and token indices\n",
    "            # Since we flatten it, we have to retrieve the actual beam indices and token_indices using floor division and remainder\n",
    "            # You can try on paper; it will make sense\n",
    "            scores, indices = torch.topk(scores.reshape(-1), beam_size) #scores: [beam_size]; #indices: [beam_size]\n",
    "            beam_indices  = torch.divide   (indices, self.output_dim, rounding_mode='floor') # indices // vocab_size\n",
    "            token_indices = torch.remainder(indices, self.output_dim)                        # indices %  vocab_size\n",
    "            \n",
    "            # print(f\"{scores=}\")\n",
    "            # print(f\"{indices.shape=}\")\n",
    "            \n",
    "            # print(f\"{indices=}\")\n",
    "            # print(f\"{beam_indices=}\")\n",
    "            # print(f\"{token_indices=}\")\n",
    "            \n",
    "            # Build the next decoder input\n",
    "            # For efficiency, the trick is to concatenate all hypotheses into one string and sent to decoder at once\n",
    "            # We can later chop it ...\n",
    "            next_decoder_input = []\n",
    "            for beam_index, token_index in zip(beam_indices, token_indices):\n",
    "                # print(f\"{beam_index=}\")\n",
    "                prev_decoder_input = decoder_input[beam_index]\n",
    "                # print(f\"{prev_decoder_input=}\")\n",
    "                if prev_decoder_input[-1]==EOS_IDX:\n",
    "                    token_index = EOS_IDX # once EOS, always EOS\n",
    "                token_index = torch.LongTensor([token_index]).long().to(device)\n",
    "                next_decoder_input.append(torch.cat([prev_decoder_input, token_index]))\n",
    "                # print(\"here: \" + \" \".join([vocab.lookup_token(i) for i in next_decoder_input[-1]]) + \"; score: \" + str(scores[beam_index].item()))\n",
    "            decoder_input = torch.vstack(next_decoder_input)\n",
    "            \n",
    "            # print(f\"{decoder_input=}\")\n",
    "            \n",
    "             # If all beams are finished, and the length is at least 5, exit\n",
    "            if i > 5:\n",
    "                if (decoder_input[:, -1]==EOS_IDX).sum() == beam_size:\n",
    "                    break\n",
    "                \n",
    "        # convert the top scored sequence to a list of text tokens\n",
    "        decoder_output, _ = max(zip(decoder_input, scores), key=lambda x: x[1])\n",
    "        decoder_output = decoder_output[1:].cpu().numpy() # remove SOS\n",
    "        \n",
    "        return [vocab.lookup_token(i) for i in decoder_output if i != EOS_IDX] # remove EOS if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)        \n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        #x = [batch size, len, hid dim]\n",
    "        #mask = [batch size, 1, len, len]\n",
    "        \n",
    "        #multi attention, skip and then norm\n",
    "        _x, attention = self.self_attention(x, x, x, mask)\n",
    "        x = self.self_attn_layer_norm(x + self.dropout(_x))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        #attention = [batch size, n heads, len, len]\n",
    "    \n",
    "        #positionwise feedforward\n",
    "        _x = self.positionwise_feedforward(x)\n",
    "        x = self.ff_layer_norm(x + self.dropout(_x))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "hid_dim    = 256                \n",
    "dec_layers = 3               \n",
    "dec_heads  = 8\n",
    "dec_pf_dim = 512\n",
    "dec_dropout = 0.1     \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,675,865 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = Decoder(vocab_size, hid_dim, dec_layers, dec_heads, dec_pf_dim, dec_dropout, device, PAD_IDX).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, bunch of tokens]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "        \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, _ = model(src)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    decoded_batch_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            #target = [batch size, dec len]\n",
    "\n",
    "            batch_size= src.shape[0]\n",
    "            prediction, _ = model(src)\n",
    "            #prediction = [batch size, dec len, output_dim]\n",
    "            \n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "            \n",
    "    #decoding using beam_search as example (you don't need to put here, because beam_search is for intference)\n",
    "    decoded_batch = model.beam_decode()\n",
    "    print(\"Sample beam sentence: \" + \" \".join(decoded_batch))\n",
    "            \n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using a `ReduceLROnPlateau` learning scheduler which decreases the learning rate by a factor, if the loss don't improve by a certain epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t a <unk>\n",
      "Epoch: 1:\n",
      "\tTrain Perplexity: 305.426\n",
      "\tValid Perplexity: 184.203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t likely to\n",
      "Epoch: 2:\n",
      "\tTrain Perplexity: 160.559\n",
      "\tValid Perplexity: 148.870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t likely to\n",
      "Epoch: 3:\n",
      "\tTrain Perplexity: 125.113\n",
      "\tValid Perplexity: 136.121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t expected to\n",
      "Epoch: 4:\n",
      "\tTrain Perplexity: 105.581\n",
      "\tValid Perplexity: 131.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t likely to\n",
      "Epoch: 5:\n",
      "\tTrain Perplexity: 92.527\n",
      "\tValid Perplexity: 129.190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t a <unk>\n",
      "Epoch: 6:\n",
      "\tTrain Perplexity: 83.389\n",
      "\tValid Perplexity: 128.418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t a <unk>\n",
      "Epoch: 7:\n",
      "\tTrain Perplexity: 76.415\n",
      "\tValid Perplexity: 129.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t likely to\n",
      "Epoch: 8:\n",
      "\tTrain Perplexity: 65.588\n",
      "\tValid Perplexity: 124.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t likely to\n",
      "Epoch: 9:\n",
      "\tTrain Perplexity: 61.336\n",
      "\tValid Perplexity: 126.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t a <unk>\n",
      "Epoch: 10:\n",
      "\tTrain Perplexity: 56.264\n",
      "\tValid Perplexity: 125.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t a <unk>\n",
      "Epoch: 11:\n",
      "\tTrain Perplexity: 53.368\n",
      "\tValid Perplexity: 124.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t a <unk>\n",
      "Epoch: 12:\n",
      "\tTrain Perplexity: 52.222\n",
      "\tValid Perplexity: 124.328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t a <unk>\n",
      "Epoch: 13:\n",
      "\tTrain Perplexity: 50.934\n",
      "\tValid Perplexity: 123.397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t a <unk>\n",
      "Epoch: 14:\n",
      "\tTrain Perplexity: 50.357\n",
      "\tValid Perplexity: 123.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t a <unk>\n",
      "Epoch: 15:\n",
      "\tTrain Perplexity: 49.773\n",
      "\tValid Perplexity: 122.859\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "seq_len  = 25 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-tr_lm-batched.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1}:')\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry <unk> is n ' t a <unk>\n",
      "Test Perplexity: 120.836\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-tr_lm-batched.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here I only use pure sampling.  You may want to put the beam search here and compare.  I will leave them as your practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, _ = model(src)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "            \n",
    "            #####################################################################\n",
    "            #I only do pure sampling....\n",
    "            #you may want to compare here with top-k, top-p, and beam search here\n",
    "            #####################################################################\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "he is a major sign that the company ' s share price\n",
      "\n",
      "0.7\n",
      "he is a good job\n",
      "\n",
      "0.75\n",
      "he is a good job\n",
      "\n",
      "0.8\n",
      "he is a good job of a handful of job and his own literature empty he was n ' t surprising\n",
      "\n",
      "1.0\n",
      "he is a third of the authors of his clients there in this single office of making for judges who are about the agency said\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'he is a '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
