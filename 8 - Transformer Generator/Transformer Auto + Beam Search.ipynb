{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer and Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from queue import PriorityQueue\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data - Wiki Text\n",
    "\n",
    "We will be using wikitext which contains a large corpus of text, perfect for language modeling task.  This time, we will use the `datasets` library from HuggingFace to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ptb_text_only (/home/st122934/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75357cd991e402a8779c595907105ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence'],\n",
      "        num_rows: 42068\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence'],\n",
      "        num_rows: 3761\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence'],\n",
      "        num_rows: 3370\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "#there are raw and preprocessed version; we used the raw one and preprocessed ourselves for fun\n",
    "dataset = datasets.load_dataset('ptb_text_only')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behind all the <unk> is some <unk> competition\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIf you try to change the index you might notice that sometimes there is no paragraph \\nand rather an empty string so we will have to care of that later.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['train'][333]['sentence'])\n",
    "\n",
    "'''\n",
    "If you try to change the index you might notice that sometimes there is no paragraph \n",
    "and rather an empty string so we will have to care of that later.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Simply tokenize the given text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/st122934/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-6d1e5992cb800354.arrow\n",
      "Loading cached processed dataset at /home/st122934/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-6fd08b61fe272feb.arrow\n",
      "Loading cached processed dataset at /home/st122934/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-b98755e1d2264d31.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['behind', 'all', 'the', '<unk>', 'is', 'some', '<unk>', 'competition']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "#function to tokenize\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['sentence'])}  \n",
    "\n",
    "#map the function to each example\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['sentence'], fn_kwargs={'tokenizer': tokenizer})\n",
    "print(tokenized_dataset['train'][333]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9881\n",
      "['<unk>', '<pad>', '<sos>', '<eos>', 'the', 'n', 'of', 'to', 'a', 'in', 'and', '.', \"'\", 's', 'that', 'for', '$', 'is', 'it', 'said', 'on', 'by', 'at', 'as', 'from', 'million', 'with', 'mr', 'was', 'be', 'are', 'its', 'he', 'but', 't', 'has', 'an', 'will', 'have', 'new', 'or', 'company', 'they', 'this', 'year', 'which', 'would', 'about', 'says', 'more', 'were', 'market', 'u', 'billion', 'his', 'had', 'their', 'up', 'one', 'than', 'who', 'some', 'been', 'also', 'stock', 'other', 'share', 'corp', 'not', 'we', 'inc', 'i', 'if', 'when', 'last', 'president', 'shares', 'years', 'all', 'first', 'two', 'because', 'trading', 'after', 'could', 'co', 'sales', '&', 'there', 'out', 'business', 'only', 'do', 'such', 'can', 'most', 'into', 'york', 'may', 'over']\n"
     ]
    }
   ],
   "source": [
    "## numericalizing\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=3, specials=special_symbols)   \n",
    "\n",
    "vocab.set_default_index(vocab['<unk>'])   \n",
    "print(len(vocab))                         \n",
    "print(vocab.get_itos()[:100])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vocab_beam.pkl', 'wb') as file:\n",
    "      \n",
    "    # A new file will be created\n",
    "    pickle.dump(vocab, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data \"Chaky loves eating at\", \"AIT `<eos>` I really\", \"love deep learning `<eos>`\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    # example = []\n",
    "    for example in dataset:\n",
    "        if example['tokens']:         \n",
    "            #appends eos so we know it ends....so model learn how to end...                             \n",
    "            tokens = example['tokens'].append('<eos>')   \n",
    "            #numericalize          \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size #get the int number of batches...\n",
    "    data = data[:num_batches * batch_size] #make the batch evenly, and cut out any remaining                      \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data #[batch size, bunch of tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    def __init__(self, previousNode, wordId, logProb, length):\n",
    "        self.prevNode = previousNode  #where does it come from\n",
    "        self.wordid   = wordId  #the numericalized integer of the word\n",
    "        self.logp     = logProb  #the log probability\n",
    "        self.len      = length  #the current length; first word starts at 1\n",
    "\n",
    "    def eval(self, alpha=0.7):\n",
    "        # the score will be simply the log probability penaltized by the length \n",
    "        # we add some small number to avoid division error\n",
    "        # read https://arxiv.org/abs/1808.10006 to understand how alpha is selected\n",
    "        return self.logp / float(self.len + 1e-6) ** (alpha)\n",
    "    \n",
    "    #this is the function for comparing between two beamsearchnodes, whether which one is better\n",
    "    #it is called when you called \"put\"\n",
    "    def __lt__(self, other):\n",
    "        return self.len < other.len\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.len > other.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device, pad_idx, max_length = 100):\n",
    "                \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "    \n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def make_mask(self, x):\n",
    "        \n",
    "        #x = [batch size, len]\n",
    "        \n",
    "        pad_mask = (x != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #pad_mask = [batch size, 1, 1, len]\n",
    "        \n",
    "        x_len = x.shape[1]\n",
    "        \n",
    "        sub_mask = torch.tril(torch.ones((x_len, x_len), device = self.device)).bool()\n",
    "        #sub_mask = [len, len]\n",
    "            \n",
    "        mask = pad_mask & sub_mask\n",
    "        #mask = [batch size, 1, len, len]\n",
    "        \n",
    "        return mask \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, len]\n",
    "                \n",
    "        batch_size = x.shape[0]\n",
    "        x_len    = x.shape[1]\n",
    "        \n",
    "        #get mask here since we remove seq2seq class\n",
    "        mask   = self.make_mask(x)\n",
    "        #mask = [batch size, 1, len, len]\n",
    "\n",
    "        pos = torch.arange(0, x_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)          \n",
    "            \n",
    "        x = self.dropout((self.tok_embedding(x) * self.scale) + self.pos_embedding(pos))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, attention = layer(x, mask)\n",
    "        \n",
    "        #x = [batch size, len, hid dim]\n",
    "        #attention = [batch size, n heads, len, len]\n",
    "        \n",
    "        output = self.fc_out(x)\n",
    "        #output = [batch size, len, output dim]\n",
    "            \n",
    "        return output, attention\n",
    "\n",
    "    def beam_decode(self, src_tensor, method='beam-search'):\n",
    "        \n",
    "        #src_tensor      = [batch size, src len]\n",
    "        src_len = src_tensor.shape[1]\n",
    "        \n",
    "        #how many parallel searches\n",
    "        beam_width = 3\n",
    "        \n",
    "        #how many sentence do you want to generate\n",
    "        topk = 1  \n",
    "        \n",
    "        #final generated sentence\n",
    "        decoded_batch = []\n",
    "                                        \n",
    "        # Start with the start of the sentence token\n",
    "        decoder_input = torch.LongTensor([SOS_IDX]).to(device)\n",
    "\n",
    "        # Number of sentence to generate\n",
    "        endnodes = []  #hold the nodes of EOS, so we can backtrack\n",
    "        number_required = min((topk + 1), topk - len(endnodes))\n",
    "\n",
    "        # starting node -  hidden vector, previous node, word id, logp, length\n",
    "        node = BeamSearchNode(None, decoder_input, 0, 1)\n",
    "        nodes = PriorityQueue()  #this is a min-heap\n",
    "\n",
    "        # start the queue\n",
    "        nodes.put((-node.eval(), node))  #we need to put - because PriorityQueue is a min-heap\n",
    "        qsize = 1\n",
    "\n",
    "        # start beam search\n",
    "        while True:\n",
    "            # give up when decoding takes too long\n",
    "            if qsize > 100: break\n",
    "            \n",
    "            # print(f\"{nodes.queue=}\")\n",
    "\n",
    "            # fetch the best node\n",
    "            # score is log p divides by the length scaled by some constants\n",
    "            score, n       = nodes.get()\n",
    "            decoder_input  = n.wordid\n",
    "\n",
    "            #get all the previous nodes, so to construct a complete decoder input\n",
    "            #because Transformer decoder expects the whole sentence\n",
    "            prevNode = n.prevNode\n",
    "            while prevNode != None:\n",
    "                prev_word = torch.LongTensor([prevNode.wordid]).to(device)\n",
    "                # print(f\"{prev_word=}\")\n",
    "                decoder_input = torch.cat((decoder_input, prev_word))\n",
    "                prevNode = prevNode.prevNode\n",
    "\n",
    "            inv_idx       = torch.arange(decoder_input.size(0)-1, -1, -1).long()\n",
    "            decoder_input = decoder_input[inv_idx]\n",
    "\n",
    "            # wordid is simply the numercalized integer of the word\n",
    "            current_len    = n.len\n",
    "\n",
    "            decoder_input  = decoder_input.unsqueeze(0)\n",
    "            #decoder_inpput: batch_size, src_len\n",
    "\n",
    "            if n.wordid.item() == EOS_IDX and n.prevNode != None:\n",
    "                endnodes.append((score, n))\n",
    "                # if we reached maximum # of sentences required\n",
    "                if len(endnodes) >= number_required:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # decode for one step using decoder\n",
    "            # decoder_input = SOS_IDX\n",
    "            # mask = [1, src len]\n",
    "            decoder_input = F.pad(decoder_input, pad=(0, src_len), mode='constant', value=PAD_IDX)\n",
    "            #pad because our decoder expects a whole sentence, not one token by token....\n",
    "\n",
    "#             print(f\"{current_len=}\")\n",
    "#             print(f\"{decoder_input=}\")\n",
    "\n",
    "            prediction, _ = self.forward(decoder_input)\n",
    "            #prediction   = [batch size, src len, output dim]\n",
    "\n",
    "            prediction = prediction[:, current_len, :] #get only the next word, but ignoring the padding\n",
    "            #prediction   = [batch size, output dim]\n",
    "\n",
    "            #so basically prediction is probabilities across all possible vocab\n",
    "            #we gonna retrieve k top probabilities (which is defined by beam_width) and their indexes\n",
    "            #recall that beam_width defines how many parallel searches we want\n",
    "            log_prob, indexes = torch.topk(prediction, beam_width)\n",
    "            # log_prob      = (1, beam width)\n",
    "            # indexes       = (1, beam width)\n",
    "            \n",
    "            # print(f\"{log_prob.shape}\")\n",
    "            # print(f\"{indexes.shape}\")\n",
    "\n",
    "            nextnodes = []  #the next possible node you can move to\n",
    "\n",
    "            # we only select beam_width amount of nextnodes\n",
    "            for top in range(beam_width):\n",
    "                pred_t = indexes[0, top].reshape(-1)  #reshape because wordid is assume to be []; see when we define SOS\n",
    "                log_p  = log_prob[0, top].item()\n",
    "\n",
    "                #decoder previous node, current node, prob, length\n",
    "                node = BeamSearchNode(n, pred_t, n.logp + log_p, n.len + 1)\n",
    "                score = -node.eval()\n",
    "                nextnodes.append((score, node))\n",
    "\n",
    "            # put them into queue\n",
    "            for i in range(len(nextnodes)):\n",
    "                score, nn = nextnodes[i]\n",
    "                nodes.put((score, nn))\n",
    "                # increase qsize\n",
    "            qsize += len(nextnodes) - 1\n",
    "\n",
    "\n",
    "        # Once everything is finished, choose nbest paths, back trace them\n",
    "\n",
    "        ## in case it does not finish, we simply get couple of nodes with highest probability\n",
    "        if len(endnodes) == 0:\n",
    "            endnodes = [nodes.get() for _ in range(topk)]\n",
    "\n",
    "        #look from the end and go back....\n",
    "        utterances = []\n",
    "        for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "            utterance = []\n",
    "            utterance.append(n.wordid)\n",
    "            # back trace by looking at the previous nodes.....\n",
    "            while n.prevNode != None:\n",
    "                n = n.prevNode\n",
    "                utterance.append(n.wordid)\n",
    "\n",
    "            utterance = utterance[::-1]  #reverse it....\n",
    "            utterances.append(utterance) #append to the list of sentences....\n",
    "\n",
    "        decoded_batch.append(utterances)\n",
    "\n",
    "        return decoded_batch  #(batch size, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)        \n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        #x = [batch size, len, hid dim]\n",
    "        #mask = [batch size, 1, len, len]\n",
    "        \n",
    "        #multi attention, skip and then norm\n",
    "        _x, attention = self.self_attention(x, x, x, mask)\n",
    "        x = self.self_attn_layer_norm(x + self.dropout(_x))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        #attention = [batch size, n heads, len, len]\n",
    "    \n",
    "        #positionwise feedforward\n",
    "        _x = self.positionwise_feedforward(x)\n",
    "        x = self.ff_layer_norm(x + self.dropout(_x))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "hid_dim    = 256                \n",
    "dec_layers = 3               \n",
    "dec_heads  = 8\n",
    "dec_pf_dim = 512\n",
    "dec_dropout = 0.1     \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,675,865 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = Decoder(vocab_size, hid_dim, dec_layers, dec_heads, dec_pf_dim, dec_dropout, device, PAD_IDX).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, bunch of tokens]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "        \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, _ = model(src)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    decoded_batch_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            \n",
    "            #tareget = [batch size, dec len]\n",
    "\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, _ = model(src)\n",
    "            #prediction = [batch size, dec len, output_dim]\n",
    "            \n",
    "            #decoding using beam_search as example (you don't need to put here, because beam_search is for intference)\n",
    "            decoded_batch = model.beam_decode(src, method='beam-search')\n",
    "            \n",
    "            #len(decoded_batch) = 64\n",
    "            #len(decoded_batch[0]) = 1 = number of sentence generated, i.e., topk            \n",
    "            decoded_batch_list.append(decoded_batch)\n",
    "            \n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "            \n",
    "        #this is optional; you don't have to; printing first three samples of the first batch\n",
    "        # print(\"print samples from first decode batch\")\n",
    "        # for sentence_index in decoded_batch_list[0][:3]:\n",
    "        #     decode_text_arr = [vocab.lookup_token(i) for i in sentence_index[0]]\n",
    "        #     decode_sentence = \" \".join(decode_text_arr)\n",
    "        #     print(\"pred target : {}\".format(decode_sentence))\n",
    "            \n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using a `ReduceLROnPlateau` learning scheduler which decreases the learning rate by a factor, if the loss don't improve by a certain epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 306.775\n",
      "\tValid Perplexity: 182.751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 159.675\n",
      "\tValid Perplexity: 148.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 124.548\n",
      "\tValid Perplexity: 136.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 105.177\n",
      "\tValid Perplexity: 130.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 92.390\n",
      "\tValid Perplexity: 128.211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 83.151\n",
      "\tValid Perplexity: 128.134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 76.190\n",
      "\tValid Perplexity: 127.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 70.593\n",
      "\tValid Perplexity: 128.927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 61.219\n",
      "\tValid Perplexity: 125.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 57.564\n",
      "\tValid Perplexity: 127.286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 52.829\n",
      "\tValid Perplexity: 126.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 50.189\n",
      "\tValid Perplexity: 124.935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 49.137\n",
      "\tValid Perplexity: 125.309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 47.952\n",
      "\tValid Perplexity: 124.797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 47.434\n",
      "\tValid Perplexity: 124.984\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "seq_len  = 25 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-tr_lm.pt')\n",
    "\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 123.293\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-tr_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here I only use pure sampling.  You may want to put the beam search here and compare.  I will leave them as your practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, _ = model(src)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "            \n",
    "            #####################################################################\n",
    "            #I only do pure sampling....\n",
    "            #you may want to compare here with top-k, top-p, and beam search here\n",
    "            #####################################################################\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "it is the way to fulfill the potential buyers of the size of the country ' s fund ' s n billion mark\n",
      "\n",
      "0.7\n",
      "it is the first time to fulfill the bill\n",
      "\n",
      "0.75\n",
      "it is the first time to fulfill by the u . s .\n",
      "\n",
      "0.8\n",
      "it is the third quarter\n",
      "\n",
      "1.0\n",
      "it is the third quarter ended sept . n by a net loss of $ n million or eight cents a share from $ n million or n cents a share last year\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'it is the '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
